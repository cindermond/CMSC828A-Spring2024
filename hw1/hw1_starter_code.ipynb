{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# HW1 Starter Snippets\n","\n","Unless otherwise stated, (nearly) all your questions can be answered in the huggingface ðŸ¤— documentation for\n","[transformers](https://huggingface.co/docs/transformers/index), \n","[tokenizers](https://huggingface.co/docs/tokenizers/index), \n","[datasets](https://huggingface.co/docs/datasets/index), or \n","[evaluate](https://huggingface.co/docs/evaluate/index), respectively.\n","They're absolutely awesome docs.\n","\n","\n","Huggingface downloads and caches models, datasets, and other files in a \"cache\" directory, so you need a few gigs of space somewhere for this.\n","Setting the HF_HOME env variable lets the library know where you want things to be cached. This is not the same as \"saving\" trained models - you specify those paths individually. \n","\n","This just saves you network bandwidth and download times by fetching a local copy when possible. It lives up to the cache namesake in some cool ways to avoid redoing preprocessing/computation."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import os\n","os.environ['HF_HOME'] = \"/cmlscratch/jkirchen/.cache/huggingface\""]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Basic Tokenization Operations"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/cmlscratch/jkirchen/miniconda3/envs/cmsc828a/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Turning a string into input token ids - the inputs to any transformer language model"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'input_ids': [101, 1142, 1110, 1280, 1106, 1129, 170, 4106, 4159, 8641, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"]}],"source":["encoded_input = tokenizer(\"this is going to be a fun programming assignment.\")\n","print(encoded_input)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Going back to strings"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/plain":["'[CLS] this is going to be a fun programming assignment. [SEP]'"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.decode(encoded_input[\"input_ids\"])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["You can tokenize in batches ... try out the different arguments "]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'input_ids': [101, 1142, 1110, 1280, 1106, 1129, 170, 4106, 4159, 8641, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"]}],"source":["batch_sentences = [\n","    \"But what about second breakfast?\",\n","    \"Don't think he knows about second breakfast, Pip.\",\n","    \"What about elevensies?\",\n","]\n","encoded_inputs = tokenizer(batch_sentences)\n","# encoded_input = tokenizer(batch_sentences, padding=True)\n","# encoded_input = tokenizer(batch_sentences, padding=True, truncation=True)\n","# encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n","print(encoded_input)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Basic Datasets Operations"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["from datasets import load_dataset"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We can load the train split of the MNLI dataset and see an overview of what \"shape\" it is"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Found cached dataset multi_nli (/cmlscratch/jkirchen/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39)\n"]},{"name":"stdout","output_type":"stream","text":["Dataset({\n","    features: ['promptID', 'pairID', 'premise', 'premise_binary_parse', 'premise_parse', 'hypothesis', 'hypothesis_binary_parse', 'hypothesis_parse', 'genre', 'label'],\n","    num_rows: 392702\n","})\n"]}],"source":["mnli_dataset = load_dataset(\"multi_nli\", split=\"train\")\n","print(mnli_dataset)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Inspecting an actual example"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["{'promptID': 55785,\n"," 'pairID': '55785e',\n"," 'premise': 'I burst through a set of cabin doors, and fell to the ground-',\n"," 'premise_binary_parse': '( I ( ( ( ( ( burst ( through ( ( a set ) ( of ( cabin doors ) ) ) ) ) , ) and ) ( fell ( to ( the ground ) ) ) ) - ) )',\n"," 'premise_parse': '(ROOT (S (NP (PRP I)) (VP (VP (VBP burst) (PP (IN through) (NP (NP (DT a) (NN set)) (PP (IN of) (NP (NN cabin) (NNS doors)))))) (, ,) (CC and) (VP (VBD fell) (PP (TO to) (NP (DT the) (NN ground))))) (: -)))',\n"," 'hypothesis': 'I burst through the doors and fell down.',\n"," 'hypothesis_binary_parse': '( I ( ( ( ( burst ( through ( the doors ) ) ) and ) ( fell down ) ) . ) )',\n"," 'hypothesis_parse': '(ROOT (S (NP (PRP I)) (VP (VP (VBP burst) (PP (IN through) (NP (DT the) (NNS doors)))) (CC and) (VP (VBD fell) (PRT (RP down)))) (. .)))',\n"," 'genre': 'fiction',\n"," 'label': 0}"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["mnli_dataset[10]"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We can also load SQuAD the same way"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Found cached dataset squad_v2 (/cmlscratch/jkirchen/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d)\n"]},{"name":"stdout","output_type":"stream","text":["Dataset({\n","    features: ['id', 'title', 'context', 'question', 'answers'],\n","    num_rows: 130319\n","})\n"]}],"source":["squad_dataset = load_dataset(\"squad_v2\", split=\"train\")\n","print(squad_dataset)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["And finally NER"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Found cached dataset parquet (/cmlscratch/jkirchen/.cache/huggingface/datasets/Babelscape___parquet/Babelscape--wikineural-579d1dc98d2a6b93/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"]},{"name":"stdout","output_type":"stream","text":["Dataset({\n","    features: ['tokens', 'ner_tags', 'lang'],\n","    num_rows: 92720\n","})\n"]}],"source":["wikineural_dataset = load_dataset(\"Babelscape/wikineural\", split=\"train_en\")\n","print(wikineural_dataset)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["{'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n"," 'ner_tags': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n"," 'lang': Value(dtype='string', id=None)}"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["wikineural_dataset.features"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/plain":["{'tokens': ['\"',\n","  'So',\n","  'here',\n","  'is',\n","  'the',\n","  'balance',\n","  'NBC',\n","  'has',\n","  'to',\n","  'consider',\n","  ':',\n","  'The',\n","  'Who',\n","  ',',\n","  \"'\",\n","  'Animal',\n","  'Practice',\n","  \"'\",\n","  '.'],\n"," 'ner_tags': [0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 7, 8, 0, 0, 7, 8, 0, 0],\n"," 'lang': 'en'}"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["wikineural_dataset[1]"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Useful operation : adding a derived column via map\n","\n","Note: run it again and see how the result is loaded from the cache. Cool!\n","If the input to the map and the function applied in the map are the same this should work"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# from the dataset card (https://huggingface.co/datasets/Babelscape/wikineural)\n","tag_set = {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}\n","tag_set = {v: k for k, v in tag_set.items()}"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Loading cached processed dataset at /cmlscratch/jkirchen/.cache/huggingface/datasets/Babelscape___parquet/Babelscape--wikineural-579d1dc98d2a6b93/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-04d0b942ec517e80.arrow\n"]},{"name":"stdout","output_type":"stream","text":["Dataset({\n","    features: ['tokens', 'ner_tags', 'lang', 'labels'],\n","    num_rows: 92720\n","})\n"]}],"source":["enriched_dataset = wikineural_dataset.map(lambda example: {'labels': [tag_set[tag] for tag in example['ner_tags']]})\n","print(enriched_dataset)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/plain":["{'tokens': ['\"',\n","  'So',\n","  'here',\n","  'is',\n","  'the',\n","  'balance',\n","  'NBC',\n","  'has',\n","  'to',\n","  'consider',\n","  ':',\n","  'The',\n","  'Who',\n","  ',',\n","  \"'\",\n","  'Animal',\n","  'Practice',\n","  \"'\",\n","  '.'],\n"," 'ner_tags': [0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 7, 8, 0, 0, 7, 8, 0, 0],\n"," 'lang': 'en',\n"," 'labels': ['O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'B-ORG',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'B-MISC',\n","  'I-MISC',\n","  'O',\n","  'O',\n","  'B-MISC',\n","  'I-MISC',\n","  'O',\n","  'O']}"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["enriched_dataset[1]"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Basic Transformers Usage\n","\n","Loading different task-specific variants of a BERT Model "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["These blocks are the most basic (and automagic) way to load a BERT model with a classification head for\n","tasks such as MNLI, extractive QA (MRC) on data like SQuAD, and token-wise classification for tasks like NER."]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["from transformers import (AutoConfig, \n","                          AutoTokenizer, \n","                          AutoModelForSequenceClassification, \n","                          AutoModelForQuestionAnswering,\n","                          AutoModelForTokenClassification)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["\n","We start with an identifier on huggingface.co/models\n","\n","For this assignment, generally, this is just the name for the standard base BERT encoder model (with a masked language model head by default)\n","\n","Teh fact that it's the pretrained model name for just the base/core model impacts the subsequent load calls below...\n"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["hf_model_name_or_path = \"bert-base-cased\"\n","\n","# Load the model configuration\n","config = AutoConfig.from_pretrained(hf_model_name_or_path)\n","# passing the num_labels argument is optional, as it can/should be inferred from the dataset\n","\n","# Load the tokenizer for this model\n","tokenizer = AutoTokenizer.from_pretrained(hf_model_name_or_path)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Take a look at the messages that appear, they are very informative about the relationship between the BERT encoder weights and the task head"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# Load the sequence classification/NLI model\n","seq_cls_model = AutoModelForSequenceClassification.from_pretrained(hf_model_name_or_path, config=config)"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# Load the MRC/question answering model\n","qa_model = AutoModelForQuestionAnswering.from_pretrained(hf_model_name_or_path, config=config)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# Load the token classification/NER model\n","ner_model = AutoModelForTokenClassification.from_pretrained(hf_model_name_or_path, config=config)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Some static versions of the outputs of these types of BERT with task head models\n","\n","To produce these yourself, run one of the `no_trainer` scripts included and stop in the training batches loop to inspect `outputs`"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["\n","### Example output for seq_cls_model (like for NLI):\n","\n","```sh\n","SequenceClassifierOutput(loss=tensor(0.7699, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1040, -0.6909,  0.4659],\n","        [-0.9791,  0.7396, -0.0732],\n","        [-0.3110, -0.6021,  1.0554],\n","        [ 1.7014, -0.6825, -1.1260],\n","        [-1.2129,  0.5246,  0.7338],\n","        [-1.5830, -0.4266,  2.4691],\n","        [-0.7301,  1.1808, -0.6139],\n","        [ 1.8110, -1.2814, -0.7678],\n","        [ 1.9778, -1.0482, -1.2913],\n","        [-1.0419,  0.1376,  1.0747],\n","        [-1.4279,  1.4548,  0.5795],\n","        [ 1.4805, -0.8703, -0.8966],\n","        [ 1.8836, -0.9205, -1.1406],\n","        [ 1.6764, -0.8014, -0.5330],\n","        [ 1.5336, -0.7486, -1.1418],\n","        [-1.2788, -1.4566,  2.5621],\n","        [-1.5375,  1.5254,  0.3369],\n","        [-0.9993,  0.6337,  0.5967],\n","        [ 0.1783, -0.3170,  0.0619],\n","        [ 1.2508, -1.3081, -0.3031],\n","        [-1.7973,  0.0338,  1.7097],\n","        [-0.6461, -0.6172,  1.2520],\n","        [ 0.1375, -0.9910,  1.1393],\n","        [-0.7546,  0.8241, -0.2152],\n","        [-1.3073,  0.8289, -0.0564],\n","        [-1.8356,  0.6909,  1.2432],\n","        [-1.6042,  0.2484,  2.3209],\n","        [ 1.3790, -0.7681, -1.0276],\n","        [-1.5430, -0.7665,  3.1297],\n","        [-1.4621,  1.5816,  0.4027],\n","        [-0.5484,  0.0363,  0.4930],\n","        [ 1.3508, -1.0034, -0.7355]], device='cuda:0',\n","       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n","```\n","\n","For a 3 class problem, the shape of the `logits` tensor is `(bsz x num_classes)`, which here is `(32,3)`\n","The `loss` is the average CE multiclass classification loss taken over the batch dim.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Example output shape for qa_model (for SQuAD MRC):\n","\n","```sh\n","QuestionAnsweringModelOutput(loss=tensor(5.8717, device='cuda:0', grad_fn=<DivBackward0>), start_logits=tensor([[-0.2344, -0.1153, -0.3226,  ..., -0.2264, -0.0654, -0.2667],\n","        [-0.0394, -0.1783, -0.1586,  ..., -0.2669, -0.1833, -0.5015],\n","        [-0.0489,  0.3059,  0.0471,  ..., -0.2551, -0.3042, -0.2128],\n","        ...,\n","        [-0.1104, -0.3046,  0.2381,  ..., -0.0628, -0.1989, -0.0310],\n","        [-0.1292, -0.4169, -0.3258,  ..., -0.1559, -0.1705, -0.1658],\n","        [-0.3442, -0.0563, -0.1119,  ..., -0.2945, -0.3721, -0.2272]],\n","       device='cuda:0', grad_fn=<CloneBackward0>), end_logits=tensor([[-0.5101, -0.2102,  0.3540,  ..., -0.1394, -0.0127, -0.0557],\n","        [-0.5998,  0.1171,  0.0030,  ..., -0.1966, -0.1890, -0.1973],\n","        [-0.4303, -0.2212,  0.7105,  ..., -0.4232, -0.2996, -0.2604],\n","        ...,\n","        [-0.2798, -0.3308,  0.2352,  ..., -0.0017, -0.0638,  0.0833],\n","        [-0.2013,  0.0410,  0.2185,  ..., -0.0690, -0.0288,  0.1315],\n","        [-0.4846, -0.0881, -0.2841,  ..., -0.3322, -0.1482, -0.3584]],\n","       device='cuda:0', grad_fn=<CloneBackward0>), hidden_states=None, attentions=None)\n","```\n","\n","For the span prediction head, the model produces two output tensors for every input text/question.\n","These have shape `(bsz x seq_length)` and give the model's prediction for the start position and end position of the answer span in the input sequence (which includes the \"passage/context\" being read). In this example they are `(12, 384)`\n","\n","The `loss` in the output object (`total_loss`) is the CE classification loss between the start/end logits and the true start/end_positions (labels), averaged over the batch dim:\n","\n","```python\n","loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n","start_loss = loss_fct(start_logits, start_positions)\n","end_loss = loss_fct(end_logits, end_positions)\n","total_loss = (start_loss + end_loss) / 2\n","```"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Example output shape for an NER model"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["```python\n","TokenClassifierOutput(loss=tensor(2.1217, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[[ 0.2860, -0.2712,  0.2737,  ..., -0.3259,  0.2023, -0.0978],\n","         [ 0.7092, -0.0134, -0.1322,  ..., -0.2983,  0.3161, -0.0509],\n","         [ 0.4163, -0.5686, -0.0563,  ...,  0.0067,  0.3946, -0.2604],\n","         ...,\n","         [ 0.2332,  0.1479,  0.2424,  ..., -0.1677,  0.0181,  0.0685],\n","         [ 0.5098, -0.0690,  0.2054,  ..., -0.4173,  0.1323, -0.1845],\n","         [ 0.4707, -0.1589,  0.1457,  ..., -0.3499,  0.2403, -0.1300]],\n","\n","        [[ 0.1392, -0.1065,  0.3485,  ..., -0.5171, -0.0702,  0.2156],\n","         [-0.1113,  0.2093,  0.2295,  ...,  0.1406,  0.0407, -0.4870],\n","         [-0.3515, -0.1806,  0.1416,  ...,  0.1510,  0.2850, -0.4419],\n","         ...,\n","         [-0.0244, -0.2980,  0.2082,  ..., -0.3929,  0.2233, -0.3086],\n","         [-0.1493, -0.1583,  0.0858,  ..., -0.3656, -0.1608,  0.0397],\n","         [-0.0885,  0.1128, -0.2955,  ..., -0.3246, -0.2501, -0.1332]],\n","\n","        [[-0.4092, -0.6252, -0.0032,  ..., -0.4688,  0.4135, -0.3822],\n","         [ 0.1071,  0.2398,  0.1925,  ..., -0.6402,  0.1205, -0.6248],\n","         [ 0.5574,  0.0604, -0.0136,  ..., -0.3985,  0.3627, -0.3857],\n","         ...,\n","         [ 0.3345, -0.6312, -0.1251,  ..., -0.1982,  0.3637, -0.8203],\n","         [ 0.2451, -0.2064,  0.0295,  ..., -0.1030,  0.4884, -0.1153],\n","         [ 0.1495,  0.0738, -0.1557,  ..., -0.0072,  0.4927,  0.6017]],\n","\n","        ...,\n","\n","        [[-0.3878,  0.0331, -0.2140,  ..., -0.3954,  0.0785, -0.1093],\n","         [-0.2488, -0.3900, -0.3403,  ..., -0.1708, -0.2019, -0.0287],\n","         [-0.2760,  0.3549,  0.0586,  ..., -0.1434,  0.1379,  0.1815],\n","         ...,\n","         [-0.1993, -0.4495,  0.0578,  ..., -0.4326,  0.2391, -0.2390],\n","         [-0.0191, -0.4702,  0.0100,  ..., -0.3972,  0.1565, -0.3812],\n","         [-0.1653, -0.0455, -0.3373,  ..., -0.2308, -0.1534, -0.1001]],\n","\n","        [[-0.1269, -0.4377, -0.0396,  ..., -0.3559,  0.0533, -0.3187],\n","         [ 0.1010, -0.3163, -0.0770,  ..., -0.3450, -0.3520, -0.0923],\n","         [ 0.1018, -0.0359,  0.2626,  ..., -0.5112,  0.3169,  0.0298],\n","         ...,\n","         [ 0.1833, -0.2909,  0.1263,  ..., -0.3456,  0.1408, -0.1342],\n","         [ 0.0845, -0.3480,  0.1911,  ..., -0.3855,  0.2223, -0.2479],\n","         [ 0.3148, -0.0760,  0.2829,  ..., -0.2543,  0.2454, -0.3342]],\n","\n","        [[-0.0261, -0.2420,  0.2130,  ..., -0.3749,  0.0549,  0.4439],\n","         [-0.0817, -0.3362, -0.1929,  ..., -0.0361,  0.2145,  0.3001],\n","         [-0.0170, -0.0779,  0.1517,  ..., -0.1094,  0.2054,  0.3024],\n","         ...,\n","         [ 0.1930, -0.2856,  0.0041,  ..., -0.0372,  0.3425,  0.1241],\n","         [ 0.0289, -0.0881, -0.0745,  ..., -0.2711,  0.2272, -0.0776],\n","         [-0.0857, -0.2791,  0.0754,  ..., -0.2199,  0.0361,  0.0086]]],\n","       device='cuda:0', grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n","```\n","\n","For token classification the logits have shape `(bsz x seq_length x num_classes)`, where the classes in the NER task are:\n","```\n","{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}\n","```\n","The `loss` is the CE multi-class classification loss comparing the predicted tag to the actual tag for each token position."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## A more assignment-specific thing with datasets..."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Get all the unique values in the genre column(Domain Splits)"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Found cached dataset multi_nli (/cmlscratch/jkirchen/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39)\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 209.89it/s]\n"]},{"data":{"text/plain":["['government', 'telephone', 'fiction', 'travel', 'slate']"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["mnli_dataset = load_dataset(\"multi_nli\")\n","\n","# drop the \"validation_mismatched\" splits\n","# mnli_dataset = mnli_dataset.remove_columns([\"validation_mismatched\"])\n","del mnli_dataset[\"validation_mismatched\"]\n","\n","# Get all the unique values in the genre column(Python)\n","# genres = set(sub_mnli_dataset[\"genre\"])\n","\n","# Get all the unique values in the genre column(NumPy arrays)\n","genres = mnli_dataset[\"train\"].unique(\"genre\")\n","genres"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Steps\n","\n","1.   We create a python dictionary to store the different datasets\n","2.   Using the genres from the previous cell, we iterate the unique genres and create a new dataset that contains only rows with the same genres\n","3. We use the DatasetDict from Hugging Face to collect all those subsets\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Loading cached processed dataset at /cmlscratch/jkirchen/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-6ad602a244b8c0fa.arrow\n","Loading cached processed dataset at /cmlscratch/jkirchen/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-8cb959e48d09b3c7.arrow\n","Loading cached processed dataset at /cmlscratch/jkirchen/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-39caa2d64fcfe2fa.arrow\n","Loading cached processed dataset at /cmlscratch/jkirchen/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-8d54e02aefc38a2f.arrow\n"]},{"data":{"text/plain":["DatasetDict({\n","    government: DatasetDict({\n","        train: Dataset({\n","            features: ['promptID', 'pairID', 'premise', 'premise_binary_parse', 'premise_parse', 'hypothesis', 'hypothesis_binary_parse', 'hypothesis_parse', 'genre', 'label'],\n","            num_rows: 77350\n","        })\n","        validation_matched: Dataset({\n","            features: ['promptID', 'pairID', 'premise', 'premise_binary_parse', 'premise_parse', 'hypothesis', 'hypothesis_binary_parse', 'hypothesis_parse', 'genre', 'label'],\n","            num_rows: 1945\n","        })\n","    })\n","    telephone: DatasetDict({\n","        train: Dataset({\n","            features: ['promptID', 'pairID', 'premise', 'premise_binary_parse', 'premise_parse', 'hypothesis', 'hypothesis_binary_parse', 'hypothesis_parse', 'genre', 'label'],\n","            num_rows: 83348\n","        })\n","        validation_matched: Dataset({\n","            features: ['promptID', 'pairID', 'premise', 'premise_binary_parse', 'premise_parse', 'hypothesis', 'hypothesis_binary_parse', 'hypothesis_parse', 'genre', 'label'],\n","            num_rows: 1966\n","        })\n","    })\n","})"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["# Create a dictionary to store the different datasets\n","genre_subsets = {}\n","\n","# Loop through the unique genres and create a new dataset for each genre\n","# just 2 for display purposes\n","genres = genres[:2]\n","for genre in genres:\n","    genre_subsets[genre] = mnli_dataset.filter(lambda example: example['genre'] == genre)\n","\n","# Collect the genre-specific datasets into Hugging Face Datasets\n","from datasets import DatasetDict\n","\n","genre_subsets_dict = DatasetDict()\n","for genre, dataset in genre_subsets.items():\n","    genre_subsets_dict[genre] = dataset\n","\n","genre_subsets_dict"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["government subset: 77350 examples\n","telephone subset: 83348 examples\n"]}],"source":["# Print the number of train examples in each genre-specific dataset\n","for genre in genres:\n","    print(f\"{genre} subset: {len(genre_subsets_dict[genre]['train'])} examples\")"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'promptID': 31193, 'pairID': '31193n', 'premise': 'Conceptually cream skimming has two basic dimensions - product and geography.', 'premise_binary_parse': '( ( Conceptually ( cream skimming ) ) ( ( has ( ( ( two ( basic dimensions ) ) - ) ( ( product and ) geography ) ) ) . ) )', 'premise_parse': '(ROOT (S (NP (JJ Conceptually) (NN cream) (NN skimming)) (VP (VBZ has) (NP (NP (CD two) (JJ basic) (NNS dimensions)) (: -) (NP (NN product) (CC and) (NN geography)))) (. .)))', 'hypothesis': 'Product and geography are what make cream skimming work. ', 'hypothesis_binary_parse': '( ( ( Product and ) geography ) ( ( are ( what ( make ( cream ( skimming work ) ) ) ) ) . ) )', 'hypothesis_parse': '(ROOT (S (NP (NN Product) (CC and) (NN geography)) (VP (VBP are) (SBAR (WHNP (WP what)) (S (VP (VBP make) (NP (NP (NN cream)) (VP (VBG skimming) (NP (NN work)))))))) (. .)))', 'genre': 'government', 'label': 1}\n"]}],"source":["print(genre_subsets_dict[\"government\"][\"train\"][0])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Finally, check out task_sampler.py for some Part 2 (and really part 1 also) inspiration\n","\n","The main method uses all the above to demo the \"task sampling\" concept to accomplish weighted multi-tasking"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## MWE of multiple BERTs with a shared backbone"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["import torch\n","from datasets import load_dataset\n","from transformers import (default_data_collator,\n","                          AutoConfig, \n","                          AutoTokenizer,\n","                          AutoModelForSequenceClassification,\n","                          AutoModelForQuestionAnswering)\n","\n","hf_model_name_or_path = \"bert-base-cased\"\n","\n","# load the tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(hf_model_name_or_path)\n","# Load the model configuration\n","nli_config = AutoConfig.from_pretrained(hf_model_name_or_path, num_labels=3, finetuning_task=\"mnli\")\n","# load the first model, whose bert encoder we'll use as a backbone\n","nli_model = AutoModelForSequenceClassification.from_pretrained(hf_model_name_or_path, config=nli_config)\n","\n","qa_config = AutoConfig.from_pretrained(hf_model_name_or_path)\n","qa_model = AutoModelForQuestionAnswering.from_pretrained(hf_model_name_or_path, config=qa_config)\n","\n","model_dict = {\n","    \"nli\": nli_model,\n","    \"qa\": qa_model\n","}\n","\n","# copy the backbone encoder from the first model\n","for model in model_dict.values():\n","    model.bert = nli_model.bert\n","    # and move to cuda\n","    model.to(\"cuda\")\n","    \n","# optionally, freeze the backbone encoder (and see the effect this has later)\n","# for param in nli_model.bert.parameters():\n","#     param.requires_grad = False"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Found cached dataset multi_nli (/cmlscratch/jkirchen/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39)\n","Found cached dataset squad (/cmlscratch/jkirchen/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"]}],"source":["# load the mnli dataset and the squad dataset\n","mnli_dataset = load_dataset(\"multi_nli\", split=\"train\")\n","squad_dataset = load_dataset(\"squad\", split=\"train\")\n","\n","# select the first 16 examples from each dataset\n","mnli_examples = mnli_dataset.select(range(4))\n","squad_examples = squad_dataset.select(range(4))"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Loading cached processed dataset at /cmlscratch/jkirchen/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-4f2cb3d6f118dc9d.arrow\n"]}],"source":["sentence1_key = \"premise\"\n","sentence2_key = \"hypothesis\"\n","padding = \"max_length\"\n","max_seq_length = 128\n","\n","def preprocess_function(examples):\n","    # Tokenize the texts\n","    texts = (\n","        (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])\n","    )\n","    result = tokenizer(*texts, padding=padding, max_length=max_seq_length, truncation=True)\n","\n","    if \"label\" in examples:\n","        result[\"labels\"] = examples[\"label\"]\n","    return result\n","\n","mnli_examples = mnli_examples.map(\n","    preprocess_function,\n","    batched=True,\n","    remove_columns=mnli_examples.column_names,\n","    desc=\"Running tokenizer on dataset\",\n",")"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Loading cached processed dataset at /cmlscratch/jkirchen/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-adc3fcf60189f060.arrow\n"]}],"source":["column_names = squad_dataset.column_names\n","pad_on_right = True\n","max_seq_length = 384\n","doc_stride = 128\n","pad_to_max_length  = True\n","\n","question_column_name = \"question\" if \"question\" in column_names else column_names[0]\n","context_column_name = \"context\" if \"context\" in column_names else column_names[1]\n","answer_column_name = \"answers\" if \"answers\" in column_names else column_names[2]\n","\n","# Training preprocessing\n","def prepare_train_features(examples):\n","    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n","    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n","    # left whitespace\n","    examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n","\n","    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n","    # in one example possible giving several features when a context is long, each of those features having a\n","    # context that overlaps a bit the context of the previous feature.\n","    tokenized_examples = tokenizer(\n","        examples[question_column_name if pad_on_right else context_column_name],\n","        examples[context_column_name if pad_on_right else question_column_name],\n","        truncation=\"only_second\" if pad_on_right else \"only_first\",\n","        max_length=max_seq_length,\n","        stride=doc_stride,\n","        return_overflowing_tokens=True,\n","        return_offsets_mapping=True,\n","        padding=\"max_length\" if pad_to_max_length else False,\n","    )\n","\n","    # Since one example might give us several features if it has a long context, we need a map from a feature to\n","    # its corresponding example. This key gives us just that.\n","    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n","    # The offset mappings will give us a map from token to character position in the original context. This will\n","    # help us compute the start_positions and end_positions.\n","    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n","\n","    # Let's label those examples!\n","    tokenized_examples[\"start_positions\"] = []\n","    tokenized_examples[\"end_positions\"] = []\n","\n","    for i, offsets in enumerate(offset_mapping):\n","        # We will label impossible answers with the index of the CLS token.\n","        input_ids = tokenized_examples[\"input_ids\"][i]\n","        cls_index = input_ids.index(tokenizer.cls_token_id)\n","\n","        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n","        sequence_ids = tokenized_examples.sequence_ids(i)\n","\n","        # One example can give several spans, this is the index of the example containing this span of text.\n","        sample_index = sample_mapping[i]\n","        answers = examples[answer_column_name][sample_index]\n","        # If no answers are given, set the cls_index as answer.\n","        if len(answers[\"answer_start\"]) == 0:\n","            tokenized_examples[\"start_positions\"].append(cls_index)\n","            tokenized_examples[\"end_positions\"].append(cls_index)\n","        else:\n","            # Start/end character index of the answer in the text.\n","            start_char = answers[\"answer_start\"][0]\n","            end_char = start_char + len(answers[\"text\"][0])\n","\n","            # Start token index of the current span in the text.\n","            token_start_index = 0\n","            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n","                token_start_index += 1\n","\n","            # End token index of the current span in the text.\n","            token_end_index = len(input_ids) - 1\n","            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n","                token_end_index -= 1\n","\n","            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n","            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n","                tokenized_examples[\"start_positions\"].append(cls_index)\n","                tokenized_examples[\"end_positions\"].append(cls_index)\n","            else:\n","                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n","                # Note: we could go after the last offset if the answer is the last word (edge case).\n","                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n","                    token_start_index += 1\n","                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n","                while offsets[token_end_index][1] >= end_char:\n","                    token_end_index -= 1\n","                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n","\n","    return tokenized_examples\n","\n","squad_examples = squad_examples.map(prepare_train_features, batched=True, remove_columns=column_names)"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["# make dataloaders for each dataset\n","from torch.utils.data import DataLoader\n","\n","mnli_dataloader = DataLoader(mnli_examples, collate_fn=default_data_collator, batch_size=4)\n","squad_dataloader = DataLoader(squad_examples, collate_fn=default_data_collator, batch_size=4)"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/cmlscratch/jkirchen/miniconda3/envs/cmsc828a/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}],"source":["# construct and optimizer for each model\n","from transformers import AdamW\n","\n","optimizers = {\n","    \"nli\": AdamW(nli_model.parameters(), lr=1e-3), # large lr to see update in single step\n","    \"qa\": AdamW(qa_model.parameters(), lr=1e-3)\n","}"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["True\n","tensor([ 0.0448, -0.0352, -0.0046, -0.0059, -0.0712], device='cuda:0',\n","       grad_fn=<SliceBackward0>)\n","tensor([ 0.0448, -0.0352, -0.0046, -0.0059, -0.0712], device='cuda:0',\n","       grad_fn=<SliceBackward0>)\n"]}],"source":["# Check that the backbone encoder is shared\n","print(nli_model.bert == qa_model.bert)\n","# verify by printing the parameter values of the last layer of the backbone encoder\n","print(nli_model.bert.encoder.layer[11].attention.self.value.weight[0, :5])\n","print(qa_model.bert.encoder.layer[11].attention.self.value.weight[0, :5])"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([ 0.0146, -0.0139,  0.0081, -0.0187, -0.0300], device='cuda:0',\n","       grad_fn=<SliceBackward0>)\n","tensor([-0.0321, -0.0021, -0.0061,  0.0225,  0.0114], device='cuda:0',\n","       grad_fn=<SliceBackward0>)\n"]}],"source":["# Check that the linear classifiers are not shared\n","print(nli_model.classifier.weight[0, :5])\n","print(qa_model.qa_outputs.weight[0, :5])"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["# take a batch from each dataloader and pas through each model, calculate the loss, and backpropagate\n","\n","# nli batch \n","mnli_batch = next(iter(mnli_dataloader))\n","mnli_batch = {k: v.to(\"cuda\") for k, v in mnli_batch.items()}\n","# qa batch\n","squad_batch = next(iter(squad_dataloader))\n","squad_batch = {k: v.to(\"cuda\") for k, v in squad_batch.items()}\n","\n","# NLI\n","nli_model.train()\n","nli_model.zero_grad()\n","nli_outputs = nli_model(**mnli_batch)\n","nli_outputs.loss\n","nli_loss = nli_outputs.loss\n","nli_loss.backward()\n","optimizers[\"nli\"].step()\n","\n","# QA\n","qa_model.train()\n","qa_model.zero_grad()\n","qa_outputs = qa_model(**squad_batch)\n","qa_loss = qa_outputs.loss\n","qa_loss.backward()\n","optimizers[\"qa\"].step()"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["True\n","tensor([ 0.0462, -0.0335, -0.0029, -0.0060, -0.0697], device='cuda:0',\n","       grad_fn=<SliceBackward0>)\n","tensor([ 0.0462, -0.0335, -0.0029, -0.0060, -0.0697], device='cuda:0',\n","       grad_fn=<SliceBackward0>)\n"]}],"source":["# check that the backbone encoder is shared and still equal, but with different params than before\n","print(nli_model.bert == qa_model.bert)\n","print(nli_model.bert.encoder.layer[11].attention.self.value.weight[0, :5])\n","print(qa_model.bert.encoder.layer[11].attention.self.value.weight[0, :5])"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([ 0.0136, -0.0129,  0.0091, -0.0197, -0.0290], device='cuda:0',\n","       grad_fn=<SliceBackward0>)\n","tensor([-0.0311, -0.0011, -0.0071,  0.0235,  0.0124], device='cuda:0',\n","       grad_fn=<SliceBackward0>)\n"]}],"source":["# Check that the linear classifiers are not shared, and updated\n","print(nli_model.classifier.weight[0, :5])\n","print(qa_model.qa_outputs.weight[0, :5])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"ce67cf325684bb5d4de4c5cbc74feb32563ba027fca5fecfd3871ce09d8c53f1"}}},"nbformat":4,"nbformat_minor":2}
